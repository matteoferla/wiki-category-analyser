# wiki-category-analyser

> This may not be the best option for large categories. See footnotes.

> It appears I did not do a good enough Google search and reinvented the wheel partially:
> https://stackoverflow.com/questions/8088226/content-of-infobox-of-wikipedia/21107068#21107068
> However my code aims retrieves all pages in a category and get their infoboxes
> https://github.com/earwig/mwparserfromhell

Given a Wikipedia category whose articles have an infobox, make a table of all the page within that category 
with fields from the infobox.
Basically, it's a tool to datamine Wikipedia. This is mostly the same code as used in:

* https://github.com/matteoferla/Wikipedia_star
* https://github.com/matteoferla/Wikipedia_Mars
* https://github.com/matteoferla/Wikipedia_dinosaurs
* https://github.com/matteoferla/Wikipedia_planes

Some infoboxes have their own autogenerated category: say 'Category:Articles_using_infobox_university' below.
But in some cases one may want a different subset.

## Usage

First find a Wikipedia article that has an infobox.
An infobox is the right hand side box with details.

Say I want all university infoboxes:

* I would browse to `University of Oxford`
* have a deep gander at the content of the box
* alter the url from `https://en.wikipedia.org/wiki/University_of_Oxford` to `https://en.wikipedia.org/w/index.php?action=raw&title=University_of_Oxford`
* look for what the infobox template is called, in this case it's `Infobox university`
* go to `https://en.wikipedia.org/wiki/Template:Infobox_university` (note how the space is URL escaped)
* have a deep gander at the contents, especially the yankie vs. brit spelling
* find the link to pages of articles with the template, i.e. `https://en.wikipedia.org/wiki/Category:Articles_using_infobox_university`

Now that we have a category let's do:

```python
from wiki_category_analyser import WikicatParser
category_name ='Articles_using_infobox_university'
pages = WikicatParser(category_name)
pages.get_pages_recursively()
# print(pages.data) # dict of page names --> data
table = pages.to_dataframe()
```
## Large categories

If there are a lot of pages (thousands) in the category, then it may be best to datamine locally.

Wikipedia can be downloaded locally: 
[English dump](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2).

```python
import mwxml, bz2

template_name = 'ðŸ‘¾ðŸ‘¾ðŸ‘¾'

remove_template_names = ['ðŸ‘¾ðŸ‘¾1', 'ðŸ‘¾ðŸ‘¾2']

dump = mwxml.Dump.from_file(bz2.open('dumps/enwiki-latest-pages-articles.xml.bz2',
                                     mode='rt', encoding='utf-8')
                           )

import re, functools

def remove_cat(text, category):
    return re.sub(r'\{\{'+category+r'[^{}]*\}\}', '', text)

print(dump.site_info.name, dump.site_info.dbname)

for page in dump:
    revision = next(page)
    if revision.text is None:
        continue
    if template_name not in revision.text:
        continue
    text = revision.text.replace('\n','').replace('{{!}}', '')
    cleaned_text = functools.reduce(remove_cat, remove_template_names, text)
    ...
```

For visitors a different approach is required. See [pagecounts](page_counts.md)


## Note on parsers

There are several parsers in the interwebs.
This repo uses `wikitextparser`, but another option is `wiki_category_analyser`
for example.
The former is a bit more convoluted while the latter seems to make mistakes with nested templates.

However, parsing templates from scratch just requires 
a bit of thinking in order to match closing brackets correctly. 
